sites: [localhost]
# sites can be set to a comma separated list of remote sites you have access to

# Default site for examples 1-3
# This site runs tasks on the local machine
site.localhost {
    execution {
        type    : "local"                            # Execution is local
        URL     : "localhost"                        # Point at localhost to run locally
    }
    staging             : direct                     # Files are on the same machine, so can be accessed "directly"
    workDirectory       : "/tmp/"${env.USER}"/swiftwork" # Directory where work is done
    maxParallelTasks    : 101                         # Maximum number of parallel tasks
    initialParallelTasks: 100                         # Maximum number of tasks at start
    app.ALL { executable: "*" }                       # All tasks to be found from commandline
}

# Instructions for Beagle
# 1. If you are running on the beagle login nodes, set jobManager: "local:pbs"
# 2. Find your project name/allocation and set jobProject : "YOUR_PROJECT_ON_BEAGLE"
# 3. Set userHomeOverride : "/lustre/beagle2/YOUR_USERNAME_ON_BEAGLE/swiftwork"
# 4. Set workDirectory : "/tmp/YOUR_USERNAME_ON_BEAGLE/swiftwork"
site.beagle {
    execution {
        type      : "coaster"                         # Use coasters to run on remote sites
        URL       : "login.beagle.ci.uchicago.edu"    # Beagle login URL
        jobManager: "ssh-cl:pbs"                      # use ssh-cl to connect, pbs is the Local Resource manager(LRM)
        options {
            maxJobs         : 1                       # Max jobs submitted to LRM
            nodeGranularity : 1                       # Nodes per job
            maxNodesPerJob  : 1                       # Nodes per job
            tasksPerNode    : 4                       # Tasks per Node
            jobQueue        : "development"           # Select queue
            jobProject      : ${env.BEAGLE_PROJECT}   # Project|Allocation on Beagle
            userHomeOverride: "/lustre/beagle2/"${env.BEAGLE_USERNAME}"/swiftwork" # Point to lustre shared-filesystem
            maxJobTime      : "00:25:00"              # Time requested per job
            jobOptions {
                pbs.aprun: true                       # Submit jobs via aprun mechanism
                pbs.mpp  : true                       # Mpp enabled
                depth    : "4"                        # 4 cores per task
            }
        }
    }
    staging             : "local"                     # Stage files from "local" system to Beagle
    workDirectory       : "/tmp/"${env.BEAGLE_USERNAME}"/swiftwork" # Location for intermediate files
    maxParallelTasks    : 101                         # Maximum number of parallel tasks
    initialParallelTasks: 100                         # Maximum number of tasks at start
    app.ALL { executable: "*" }                       # All tasks to be found from commandline
}

# Instructions for Midway:
# 1. If you are running on the midway login nodes you set jobManager: "local:slurm"
# 2. Set workDirectory to /tmp/your_username_on_midway
site.midway {
    execution {
        type      : "coaster"                         # Use coasters to run on remote sites
        URL       : "swift.rcc.uchicago.edu"          # Midway login node
        jobManager: "ssh-cl:slurm"                    # Use ssh-cl to connect, slurm is the Local resource manager
        options {
            maxJobs         : 1                       # Max jobs submitted to LRM
            nodeGranularity : 1                       # Nodes per job
            maxNodesPerJob  : 1                       # Nodes per job
            tasksPerNode    : 1                       # Tasks per Node
            jobQueue        : "sandyb"                # Select queue from (sandyb, westmere, ...)
            maxJobTime      : "00:25:00"              # Time requested per job
        }
    }
    staging             : "local"                     # Stage files from "local" system to Midway
    workDirectory       : "/tmp/"${env.MIDWAY_USERNAME} # Location for intermediate files
    maxParallelTasks    : 101                         # Maximum number of parallel tasks
    initialParallelTasks: 100                         # Maximum number of tasks at start
    app.ALL { executable: "*" }                       # All tasks to be found from commandline
}

# Instruction for OSGConnect
# 1. If you are running on the OSGConnect login node set jobManager: "local:condor"
# 2. Set projectname : "YOUR_PROJECTNAME_ON_OSG"
# 3. Set workDirectory : "/tmp/YOUR_USERNAME_ON_OSG"
site.osgc {
    execution {
        type       : "coaster"                        # Use coasters to run on remote sites
        URL        : "login.osgconnect.net"           # OSGConnect login node
        jobManager : "ssh-cl:condor"                  # Use ssh-cl to connect, Condor is the Local resource manager
        options {
            maxJobs         : 1                       # Max jobs submitted to LRM
            nodeGranularity : 1                       # Nodes per job
            maxNodesPerJob  : 1                       # Nodes per job
            tasksPerNode    : 1                       # Tasks per Node
            maxJobTime      : "00:25:00"              # Time requested per job
            jobOptions.condor {
                projectname  : ${env.OSG_PROJECT}     # Set the Project name of your allocation on OSG
# Using requirements directive, you can add any requirement attributes for condor. Eg:
#               requirements    : "(HAS_CVMFS_oasis_opensciencegrid_org =?= TRUE)"
            }
        }
    }
    staging             : "local"                     # Stage files from "local" system to OSGConnect
    workDirectory       : "/tmp/"${env.OSG_USERNAME}  # Location for intermediate files
    maxParallelTasks    : 101                         # Maximum number of parallel tasks
    initialParallelTasks: 100                         # Maximum number of tasks at start
    app.ALL { executable: "*" }                       # All tasks to be found from commandline
}

# Instructions for Amazon EC2
# 1. Set ec2CredentialsFile: "ABSOLUTE_PATH_TO_YOUR_AWS_CREDENTIALS_FILE"
# 2. Ensure that ec2KeypairFile points to a valid path
# 3. If you update ec2WorkerImage, ensure that it is a ubuntu image
site.aws {
    execution {
        type       : "coaster"                        # Use coasters to run on remote sites
        URL        : "127.0.0.1"                      # We use a local service to connect to AWS
        jobManager : "local:ec2-cloud"                # ec2-cloud provider manages the cloud resources
        options {
            maxJobs         : 1                       # Max jobs requested from AWS
            tasksPerNode    : 2                       # Tasks per Node
            maxJobTime      : "00:25:00"              # Time requested per job
            jobOptions {
                ec2CredentialsFile: ${env.AWS_CREDENTIALS_FILE} # The credentials required to authenticate with AWS
                ec2SecurityGroup  : swift_security_group        # This security group will be generated
                ec2KeypairName    : swift-test-pair             # Key-pair to connect to nodes
                ec2KeypairFile    : ${env.HOME}/.ssh/swift-test-pair.pem # Path to create the key-pair
                ec2WorkerImage    : ami-23700813                # Amazon image id
                ec2WorkerType     : t1.micro                    # Instance type
            }
        }
    }
    staging             : "local"                     # Stage files from "local" system to AWS
    workDirectory       : "/tmp/"${env.USER}"/work"   # Location for intermediate files
    maxParallelTasks    : 20                          # Maximum number of parallel tasks
    initialParallelTasks: 20                          # Maximum number of tasks at start
    app.ALL { executable: "*" }                       # All tasks to be found from commandline
}


# Instructions for a pool of Ad-hoc machines
# Assume that you have access to N machines ( alpha.foo.net, beta.foo.net ... omega.foo.net )
# Replicate the site.ad-hoc-1 site definition block for each site you have access to.
# Set the site.<SITE_NAME> with say site.alpha, site.beta etc..
# Set the site definition for each site with the correct URL
# Set the first line of this file to a comma separated list of sites, eg:
# sites: [ alpha, beta, omega ]

# Instctions for Ad-hoc-1
# 1. Set URL : URL_OF_AD_HOC_MACHINE_1
# 2. Set workDirectory : "/tmp/USERNAME_ON_AD-HOC-1/work"
site.ad-hoc-1 {
    execution {
        type       : "coaster"                        # Use coasters to run on remote sites
        URL        : ${env.URL_OF_AD_HOC_MACHINE_1}   # URL of the remote machine to connect to
        jobManager : "ssh-cl:local"                   # ssh-cl to connect to machine, run worker locally
        options {
            maxJobs         : 2                       # Max jobs sent to remote node
            tasksPerNode    : 10                      # Tasks per Node
            maxJobTime      : "00:25:00"              # Time requested per job
        }
    }
    staging             : "local"                     # Stage files from "local" system to ad-hoc-1
    workDirectory       : "/tmp/"${env.USER}"/work"   # Location for intermediate files
    maxParallelTasks    : 20                          # Maximum number of parallel tasks
    initialParallelTasks: 20                          # Maximum number of tasks at start
    app.ALL { executable: "*" }                       # All tasks to be found from commandline
}

# Instructions for Ad-hoc site
# 1. Set URL : URL_OF_AD_HOC_MACHINE_N
# 2. Set workDirectory : "/tmp/USERNAME_ON_AD-HOC-N/work"
site.ad-hoc-N {
    execution {
        type       : "coaster"                        # Use coasters to run on remote sites
        URL        : ${env.URL_OF_AD_HOC_MACHINE_N}   # URL of the remote machine to connect to
        jobManager : "ssh-cl:local"                   # ssh-cl to connect to machine, run worker locally
        options {
            maxJobs         : 2                       # Max jobs sent to remote node
            tasksPerNode    : 10                      # Tasks per Node
            maxJobTime      : "00:25:00"              # Time requested per job
        }
    }
    staging             : "local"                     # Stage files from "local" system to ad-hoc-N
    workDirectory       : "/tmp/"${env.USER}"/work"   # Location for intermediate files
    maxParallelTasks    : 20                          # Maximum number of parallel tasks
    initialParallelTasks: 20                          # Maximum number of tasks at start
    app.ALL { executable: "*" }                       # All tasks to be found from commandline
}

TCPPortRange: "50000,51000"                           # TCP port range used by swift to communicate with remote sites
lazyErrors: false                                     # Swift fails immediately upon encountering an error
executionRetries: 0                                   # Set number of retries upon task failures
keepSiteDir: true                                     # Keep Site Dir (useful for debug)
providerStagingPinSwiftFiles: false                   # Pin staging files (useful for debug)
alwaysTransferWrapperLog: true                        # Transfer wrapper logs (useful for debug)
